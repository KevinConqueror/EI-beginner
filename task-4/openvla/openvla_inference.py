# -*- coding: utf-8 -*-
"""openvla_inference.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SJ4xY2ZIcC5GfKQW-L1tQUEMJWL-dG2P
"""

!pip install flash-attn
!pip uninstall -y timm
!pip install timm==0.9.16

!nvidia-smi

import torch
import numpy as np
from PIL import Image
from transformers import AutoModelForVision2Seq, AutoProcessor

# =========================
# 1. Mock Robot Interface
# =========================

class MockRobot:
    """
    用来模拟真实机器人控制接口
    """
    def act(self, action: np.ndarray):
        assert action.shape == (7,)
        print("Executing action:")
        print(f"  Δx, Δy, Δz = {action[:3]}")
        print(f"  Δroll, Δpitch, Δyaw = {action[3:6]}")
        print(f"  gripper = {action[6]}")


# =========================
# 2. Load OpenVLA
# =========================

def load_openvla(device="cuda:0"):
    processor = AutoProcessor.from_pretrained(
        "openvla/openvla-7b",
        trust_remote_code=True
    )

    model = AutoModelForVision2Seq.from_pretrained(
        "openvla/openvla-7b",
        attn_implementation="eager",# "flash_attention_2",
        torch_dtype=torch.bfloat16,
        low_cpu_mem_usage=True,
        trust_remote_code=True
    ).to(device)

    model.eval()
    return processor, model


# =========================
# 3. Get Image (Camera / File)
# =========================

def get_image_from_file(path: str) -> Image.Image:
    """
    模拟从相机获取一帧
    """
    img = Image.open(path).convert("RGB")
    return img


# =========================
# 4. Inference with OpenVLA
# =========================

@torch.no_grad()
def predict_action(
    processor,
    model,
    image: Image.Image,
    instruction: str,
    device="cuda:0"
):
    prompt = (
        "In: What action should the robot take to "
        f"{instruction}?\n"
        "Out:"
    )

    inputs = processor(
        prompt,
        image,
        return_tensors="pt"
    ).to(device, dtype=torch.bfloat16)

    # OpenVLA 专用 API（不是 generate）
    action = model.predict_action(
        **inputs,
        unnorm_key="bridge_orig",  # BridgeData V2 反归一化
        do_sample=False
    )

    # torch.Tensor -> numpy
    return action

# =========================
# 5. Main Script
# =========================

def main():
    device = "cuda:0"
    robot = MockRobot()

    print("Loading OpenVLA...")
    processor, model = load_openvla(device)

    print("Loading image...")
    image = get_image_from_file("scene.jpg")

    instruction = "pick up the red block and place it on the blue square"

    print("Running OpenVLA inference...")
    action = predict_action(
        processor,
        model,
        image,
        instruction,
        device
    )

    print("Action predicted:", action)

    print("Sending to robot...")
    robot.act(action)


if __name__ == "__main__":
    main()

